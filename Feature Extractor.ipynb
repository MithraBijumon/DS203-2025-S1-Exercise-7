{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5de2996-808a-44f3-8921-4b0c8e343189",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33466722-8875-4daa-ad25-b1f77231a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|████████████████████████████████████████████████████████████████████████| 465/465 [01:31<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image CIMG0579(2).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0579.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0580.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0584.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0585.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0593.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0594.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0595.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0599.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0601.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0602.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0604.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0605.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0608.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0642.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0672.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0757(1).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0757(2).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0757.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0914(1).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0914.jpg. Skipping.\n",
      "Warning: Could not load image CIMG0958.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0975.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0979.jpg. Skipping.\n",
      "Warning: Could not load image CIMG0980(1).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0980.jpg. Skipping.\n",
      "Warning: Could not load image CIMG1147.JPG. Skipping.\n",
      "Warning: Could not load image CIMG1270.JPG. Skipping.\n",
      "Warning: Could not load image DSC00095.JPG. Skipping.\n",
      "Warning: Could not load image DSC00096.JPG. Skipping.\n",
      "Warning: Could not load image DSC00182.JPG. Skipping.\n",
      "Warning: Could not load image DSC00183.JPG. Skipping.\n",
      "Warning: Could not load image DSC00186.JPG. Skipping.\n",
      "Warning: Could not load image DSC00319.JPG. Skipping.\n",
      "Warning: Could not load image DSC00320.JPG. Skipping.\n",
      "Warning: Could not load image DSC00322.JPG. Skipping.\n",
      "Warning: Could not load image DSC00342.JPG. Skipping.\n",
      "Warning: Could not load image DSC00343.JPG. Skipping.\n",
      "Warning: Could not load image DSC00344.JPG. Skipping.\n",
      "Warning: Could not load image DSC00386.JPG. Skipping.\n",
      "Warning: Could not load image DSC02847.JPG. Skipping.\n",
      "Warning: Could not load image DSC02848.JPG. Skipping.\n",
      "Warning: Could not load image DSC_0359.JPG. Skipping.\n",
      "Warning: Could not load image DSC_0360.JPG. Skipping.\n",
      "Warning: Could not load image IMG_1724.JPG. Skipping.\n",
      "Warning: Could not load image IMG_2510.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3332(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3332.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3334(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3334.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3337.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3338.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3347.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3348.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3361.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3368.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3369(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3369.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3370.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3371.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3372.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3373.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3378.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3385.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3386.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3387.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3388.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3393.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3394.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3395.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3403(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3403.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3404.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3406.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3409.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3410(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3410.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3411.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3412.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3416.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3417.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3418.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3419.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3438.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3439.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3477.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3550.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3569(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3637.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3653.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3674.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3676.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3682.JPG. Skipping.\n",
      "Warning: Could not load image IMG-20140317-WA0001.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180401_143225122_HDR.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180724_170028451.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180724_170036332.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180724_170112733.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154742802.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154804552.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154917733_BURST000_COVER_TOP.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154940261_BURST000_COVER_TOP.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154949063_BURST001.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20211223_163150170_HDR.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20221207_075613681.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20240822_085549698~3.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250524_140427028.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250524_143429630.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250627_104711559_HDR.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250627_105023208.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_121136381.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_121136381~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_125343397.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_125546309.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_142522.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_170544521.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250708_120702744_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250709_163052484_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250709_164031681_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250709_164054571_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250710_100837.jpg. Skipping.\n",
      "Warning: Could not load image IMG_3686.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3687.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3699.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3700.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3701.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3706.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3747.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3748.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3761.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3839.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3844.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3858.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4131.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4142.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4150.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4158(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_4158.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4164.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4165.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4171.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4172.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4173.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4174.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4175.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4176.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4182(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_4184.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4185.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4186.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4190.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4191.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4192.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4193.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4194(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_4194.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4195.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4196.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4197.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4198.JPG. Skipping.\n",
      "Warning: Could not load image Photo0044.jpg. Skipping.\n",
      "Warning: Could not load image RIMG0047.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0050.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0051.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0066.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0075.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0076.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0079.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0081.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0124.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0919.JPG. Skipping.\n",
      "Warning: Could not load image RIMG1328.JPG. Skipping.\n",
      "Warning: Could not load image RIMG1945.JPG. Skipping.\n",
      "Warning: Could not load image Rimg0149s.jpg. Skipping.\n",
      "Warning: Could not load image p10.jpg. Skipping.\n",
      "Warning: Could not load image p12.jpg. Skipping.\n",
      "Warning: Could not load image p16.jpg. Skipping.\n",
      "Warning: Could not load image p17.jpg. Skipping.\n",
      "Warning: Could not load image p19.jpg. Skipping.\n",
      "Warning: Could not load image p24.jpg. Skipping.\n",
      "Warning: Could not load image p25.jpg. Skipping.\n",
      "Warning: Could not load image p4.jpg. Skipping.\n",
      "Warning: Could not load image p6.jpg. Skipping.\n",
      "Warning: Could not load image p7.jpg. Skipping.\n",
      "Warning: Could not load image p8.jpg. Skipping.\n",
      "Warning: Could not load image p9.jpg. Skipping.\n",
      "\n",
      "Feature extraction complete. Assembling final dataset...\n",
      "Success! Training dataset saved to training_dataset_baseline.csv\n",
      "Total rows (cells): 17856\n",
      "Total features: 193\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from tqdm import tqdm # A progress bar! pip install tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PREPROCESSED_DIR = \"preprocessed_images\"\n",
    "LABELS_FILE = \"label_results_merged (1).csv\" # Your manually created labels\n",
    "OUTPUT_DATASET = \"training_dataset_baseline.csv\" # The file this script will create\n",
    "\n",
    "GRID_ROWS = 8\n",
    "GRID_COLS = 8\n",
    "CELL_HEIGHT = 600 // GRID_ROWS # 75 pixels\n",
    "CELL_WIDTH = 800 // GRID_COLS  # 100 pixels\n",
    "\n",
    "# LBP (Local Binary Patterns) configuration\n",
    "LBP_POINTS = 24 # Number of points to check around a pixel\n",
    "LBP_RADIUS = 3  # Radius of the circle\n",
    "\n",
    "# --- Color Histogram configuration ---\n",
    "HIST_BINS = [4, 4, 4] # 4 bins for B, 4 for G, 4 for R = 4*4*4=64 features\n",
    "\n",
    "# --- HOG Configuration ---\n",
    "HOG_ORIENTATIONS = 8\n",
    "HOG_PIXELS_PER_CELL = (25, 25) # 100x75 cell -> (100/25)x(75/25) = 4x3 grid of cells\n",
    "HOG_CELLS_PER_BLOCK = (1, 1)   # No block normalization\n",
    "# Total HOG Features = 4 * 3 * 1 * 1 * 8 = 96 features\n",
    "N_HOG_FEATURES = (CELL_WIDTH // HOG_PIXELS_PER_CELL[0]) * \\\n",
    "                 (CELL_HEIGHT // HOG_PIXELS_PER_CELL[1]) * \\\n",
    "                 (HOG_CELLS_PER_BLOCK[0] * HOG_CELLS_PER_BLOCK[1]) * \\\n",
    "                 HOG_ORIENTATIONS\n",
    "\n",
    "def extract_features_for_cell(cell):\n",
    "    \"\"\"\n",
    "    This function takes one 100x75 cell (a BGR image) and returns\n",
    "    a 1D numpy array of its calculated baseline features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    # --- 1. Color Features (Average & Std Dev BGR) ---\n",
    "    # We now capture both mean and standard deviation\n",
    "    (means, stds) = cv2.meanStdDev(cell)\n",
    "    features.extend(means.flatten()) # Add B_mean, G_mean, R_mean\n",
    "    features.extend(stds.flatten()) # Add B_std, G_std, R_std\n",
    "\n",
    "    # --- 2. Grayscale & Edge Features ---\n",
    "    gray_cell = cv2.cvtColor(cell, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Canny Edge Density\n",
    "    # We still need grayscale stats to find good Canny thresholds\n",
    "    (gray_mean, gray_std) = cv2.meanStdDev(gray_cell)\n",
    "    sigma = gray_std[0][0]\n",
    "    v = gray_mean[0][0]\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    \n",
    "    edges = cv2.Canny(gray_cell, lower, upper)\n",
    "    edge_density = np.sum(edges > 0) / (CELL_WIDTH * CELL_HEIGHT)\n",
    "    features.append(edge_density)\n",
    "\n",
    "    # --- 3. Texture Features (LBP) ---\n",
    "    # We use the 'uniform' method, which is robust and limits features.\n",
    "    # It results in (LBP_POINTS + 2) features.\n",
    "    lbp = local_binary_pattern(gray_cell, LBP_POINTS, LBP_RADIUS, method=\"uniform\")\n",
    "    \n",
    "    # Create a normalized histogram of the LBP results\n",
    "    (hist, _) = np.histogram(lbp.ravel(),\n",
    "                             bins=np.arange(0, LBP_POINTS + 3),\n",
    "                             range=(0, LBP_POINTS + 2))\n",
    "    \n",
    "    # Normalize histogram to be a probability distribution\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6) # Add epsilon to avoid division by zero\n",
    "    \n",
    "    features.extend(hist)\n",
    "\n",
    "    # --- 4. 3D Color Histogram ---\n",
    "    # Calculate a 3D histogram for B, G, R channels\n",
    "    hist_3d = cv2.calcHist([cell], [0, 1, 2], None, HIST_BINS,\n",
    "                           [0, 256, 0, 256, 0, 256])\n",
    "    \n",
    "    # Normalize the histogram\n",
    "    cv2.normalize(hist_3d, hist_3d, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "    \n",
    "    #Flatten the 3D histogram into a 1D vector\n",
    "    features.extend(hist_3d.flatten())\n",
    "\n",
    "    # --- 5. HOG (Histogram of Oriented Gradients) Features ---\n",
    "    hog_features = hog(gray_cell, orientations=HOG_ORIENTATIONS,\n",
    "                       pixels_per_cell=HOG_PIXELS_PER_CELL,\n",
    "                       cells_per_block=HOG_CELLS_PER_BLOCK,\n",
    "                       visualize=False, block_norm='L1')\n",
    "    features.extend(hog_features.flatten())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting feature extraction...\")\n",
    "    \n",
    "    # Load the labels\n",
    "    if not os.path.exists(LABELS_FILE):\n",
    "        print(f\"Error: Labels file not found at {LABELS_FILE}\")\n",
    "        exit()\n",
    "    labels_df = pd.read_csv(LABELS_FILE)\n",
    "    labels_df.rename(columns={\"filename\": \"image_name\"}, inplace=True)\n",
    "    new_columns = {\"cell_\" + str(i): f\"c{i+1:02d}\" for i in range(64)}\n",
    "    labels_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "    cell_indices = []\n",
    "\n",
    "    # Use tqdm for a progress bar. Iterating rows is like iterating images.\n",
    "    for index, row in tqdm(labels_df.iterrows(), total=labels_df.shape[0], desc=\"Processing Images\"):\n",
    "        image_name = row['image_name']\n",
    "        image_path = os.path.join(PREPROCESSED_DIR, image_name)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not load image {image_name}. Skipping.\")\n",
    "            continue\n",
    "        l = [image_name]*GRID_ROWS*GRID_COLS\n",
    "        all_images+=l\n",
    "            \n",
    "        # Iterate over the 8x8 grid\n",
    "        cell_index = 1\n",
    "        for i in range(GRID_ROWS):\n",
    "            for j in range(GRID_COLS):\n",
    "                # Calculate coordinates for the cell\n",
    "                y1, y2 = i * CELL_HEIGHT, (i + 1) * CELL_HEIGHT\n",
    "                x1, x2 = j * CELL_WIDTH, (j + 1) * CELL_WIDTH\n",
    "                \n",
    "                # Extract the 100x75 cell\n",
    "                cell = image[y1:y2, x1:x2]\n",
    "                \n",
    "                # Get the label for this cell from the CSV\n",
    "                # Column name is 'c01', 'c02', ... 'c64'\n",
    "                cell_label_col = f'c{(i * GRID_COLS + j + 1):02d}'\n",
    "                label = row[cell_label_col]\n",
    "                \n",
    "                # --- This is the core step ---\n",
    "                features = extract_features_for_cell(cell)\n",
    "                \n",
    "                all_features.append(features)\n",
    "                all_labels.append(label)\n",
    "                cell_indices.append(cell_index)\n",
    "                cell_index+=1\n",
    "\n",
    "    print(\"\\nFeature extraction complete. Assembling final dataset...\")\n",
    "\n",
    "    # Create a list of feature names for the CSV header\n",
    "    # This list is now updated to match our baseline features\n",
    "    feature_names = ['B_mean', 'G_mean', 'R_mean', 'B_std', 'G_std', 'R_std', 'Edge_density']\n",
    "    # Add LBP histogram feature names\n",
    "    feature_names += [f'LBP_bin_{k}' for k in range(LBP_POINTS + 2)]\n",
    "    # Add Color Histogram names (e.g., 'hist_B0_G0_R0')\n",
    "    hist_feature_names = [f'hist_B{b}_G{g}_R{r}' \n",
    "                          for b in range(HIST_BINS[0]) \n",
    "                          for g in range(HIST_BINS[1]) \n",
    "                          for r in range(HIST_BINS[2])]\n",
    "    feature_names.extend(hist_feature_names)\n",
    "    # Add HOG feature names\n",
    "    hog_feature_names = [f'hog_{i}' for i in range(N_HOG_FEATURES)]\n",
    "    feature_names.extend(hog_feature_names)\n",
    "\n",
    "    # Create the final DataFrame\n",
    "    X = np.array(all_features)\n",
    "    y = np.array(all_labels)\n",
    "    image_names = np.array(all_images)\n",
    "    cell_indices = np.array(cell_indices)\n",
    "\n",
    "    dataset_df = pd.DataFrame(X, columns=feature_names)\n",
    "    dataset_df['label'] = y\n",
    "    dataset_df['image_name'] = image_names\n",
    "    dataset_df['cell_index'] = cell_indices\n",
    "    \n",
    "    # Save to CSV\n",
    "    dataset_df.to_csv(OUTPUT_DATASET, index=False)\n",
    "    \n",
    "    print(f\"Success! Training dataset saved to {OUTPUT_DATASET}\")\n",
    "    print(f\"Total rows (cells): {len(dataset_df)}\")\n",
    "    print(f\"Total features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db323229-7348-4adb-afea-d9188ad1d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mithr\\AppData\\Local\\Temp\\ipykernel_120664\\4124604336.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0s: 20926\n",
      "Count of 1s: 8369\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for _, row in labels_df.iterrows():  \n",
    "    for i in range(1,64):\n",
    "        label = row[i]\n",
    "        labels.append(label)\n",
    "count_zeros = labels.count(0)\n",
    "count_ones = labels.count(1)\n",
    "print(f\"Count of 0s: {count_zeros}\")\n",
    "print(f\"Count of 1s: {count_ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84deb11b-e7a7-4f54-959d-f6aaf8508932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport pickle\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport os\\n\\n# --- CONFIGURATION ---\\nDATASET_FILE = \"training_dataset_baseline.csv\"\\nMODEL_OUTPUT_FILE = \"wildlife_model.pkl\" # The final, saved model\\nPLOT_OUTPUT_FILE = \"confusion_matrix.png\" # The evaluation plot\\n\\n# --- MAIN SCRIPT ---\\nif __name__ == \"__main__\":\\n\\n    # --- 1. Load Data ---\\n    print(f\"Loading dataset from {DATASET_FILE}...\")\\n    if not os.path.exists(DATASET_FILE):\\n        print(f\"Error: Dataset file not found at {DATASET_FILE}\")\\n        print(\"Please run feature_extractor.py first.\")\\n        exit()\\n\\n    df = pd.read_csv(DATASET_FILE)\\n\\n    # Simple data cleaning: Fill any potential NaN values with 0\\n    # (This can happen with LBP on all-black cells, etc.)\\n    df.fillna(0, inplace=True)\\n\\n    # --- 2. Split Data (Features & Labels) ---\\n    print(\"Splitting data into training and test sets...\")\\n\\n    X = df.drop(\"label\", axis=1) # All columns EXCEPT \\'label\\'\\n    y = df[\"label\"]             # Just the \\'label\\' column\\n\\n    # Check for class imbalance (important for your report)\\n    print(\"\\nClass Distribution:\")\\n    print(y.value_counts(normalize=True))\\n\\n    # Split the data. \\n    # test_size=0.3 means 30% for testing, 70% for training.\\n    # stratify=y is CRITICAL: it ensures your train and test sets have\\n    # the same percentage of 0s and 1s as the full dataset.\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \\n                                                        test_size=0.3, \\n                                                        random_state=42, \\n                                                        stratify=y)\\n\\n    print(f\"\\nTraining set size: {len(X_train)} cells\")\\n    print(f\"Test set size: {len(X_test)} cells\")\\n\\n    # --- 3. Train Baseline Model ---\\n    print(\"\\nTraining baseline RandomForestClassifier...\")\\n\\n    # n_estimators=100 is a good default (100 \"trees\" in the forest)\\n    # n_jobs=-1 uses all your CPU cores to speed up training\\n    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\\n\\n    # This is the step that does the actual learning\\n    model.fit(X_train, y_train)\\n\\n    print(\"Model training complete.\")\\n\\n    # --- 4. Evaluate Model ---\\n    print(\"\\nEvaluating model on the test set...\")\\n\\n    # Get predictions for the test data\\n    y_pred = model.predict(X_test)\\n\\n    # Print a full report\\n    print(\"\\n--- Classification Report ---\")\\n    print(classification_report(y_test, y_pred, target_names=[\"No Wildlife (0)\", \"Wildlife (1)\"]))\\n\\n    # Generate and plot a confusion matrix\\n    print(\"\\n--- Confusion Matrix ---\")\\n    cm = confusion_matrix(y_test, y_pred)\\n    print(cm)\\n\\n    # Create a heatmap plot of the confusion matrix\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(cm, annot=True, fmt=\\'d\\', cmap=\\'Blues\\', \\n                xticklabels=[\"Predicted 0\", \"Predicted 1\"], \\n                yticklabels=[\"Actual 0\", \"Actual 1\"])\\n    plt.title(\\'Confusion Matrix\\')\\n    plt.ylabel(\\'Actual Label\\')\\n    plt.xlabel(\\'Predicted Label\\')\\n    plt.savefig(PLOT_OUTPUT_FILE)\\n    print(f\"\\nConfusion matrix plot saved to {PLOT_OUTPUT_FILE}\")\\n\\n    # --- 5. Save the Final Model ---\\n    print(f\"\\nSaving trained model to {MODEL_OUTPUT_FILE}...\")\\n\\n    # Use pickle to save the model to a file\\n    with open(MODEL_OUTPUT_FILE, \"wb\") as f:\\n        pickle.dump(model, f)\\n\\n    print(\"\\nBaseline model pipeline complete. Model is saved and ready for prediction.\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "MODEL_OUTPUT_FILE = \"wildlife_model.pkl\" # The final, saved model\n",
    "PLOT_OUTPUT_FILE = \"confusion_matrix.png\" # The evaluation plot\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "    if not os.path.exists(DATASET_FILE):\n",
    "        print(f\"Error: Dataset file not found at {DATASET_FILE}\")\n",
    "        print(\"Please run feature_extractor.py first.\")\n",
    "        exit()\n",
    "        \n",
    "    df = pd.read_csv(DATASET_FILE)\n",
    "\n",
    "    # Simple data cleaning: Fill any potential NaN values with 0\n",
    "    # (This can happen with LBP on all-black cells, etc.)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # --- 2. Split Data (Features & Labels) ---\n",
    "    print(\"Splitting data into training and test sets...\")\n",
    "    \n",
    "    X = df.drop(\"label\", axis=1) # All columns EXCEPT 'label'\n",
    "    y = df[\"label\"]             # Just the 'label' column\n",
    "\n",
    "    # Check for class imbalance (important for your report)\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(y.value_counts(normalize=True))\n",
    "    \n",
    "    # Split the data. \n",
    "    # test_size=0.3 means 30% for testing, 70% for training.\n",
    "    # stratify=y is CRITICAL: it ensures your train and test sets have\n",
    "    # the same percentage of 0s and 1s as the full dataset.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.3, \n",
    "                                                        random_state=42, \n",
    "                                                        stratify=y)\n",
    "    \n",
    "    print(f\"\\nTraining set size: {len(X_train)} cells\")\n",
    "    print(f\"Test set size: {len(X_test)} cells\")\n",
    "\n",
    "    # --- 3. Train Baseline Model ---\n",
    "    print(\"\\nTraining baseline RandomForestClassifier...\")\n",
    "    \n",
    "    # n_estimators=100 is a good default (100 \"trees\" in the forest)\n",
    "    # n_jobs=-1 uses all your CPU cores to speed up training\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # This is the step that does the actual learning\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- 4. Evaluate Model ---\n",
    "    print(\"\\nEvaluating model on the test set...\")\n",
    "    \n",
    "    # Get predictions for the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print a full report\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"No Wildlife (0)\", \"Wildlife (1)\"]))\n",
    "    \n",
    "    # Generate and plot a confusion matrix\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Create a heatmap plot of the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[\"Predicted 0\", \"Predicted 1\"], \n",
    "                yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(PLOT_OUTPUT_FILE)\n",
    "    print(f\"\\nConfusion matrix plot saved to {PLOT_OUTPUT_FILE}\")\n",
    "\n",
    "    # --- 5. Save the Final Model ---\n",
    "    print(f\"\\nSaving trained model to {MODEL_OUTPUT_FILE}...\")\n",
    "    \n",
    "    # Use pickle to save the model to a file\n",
    "    with open(MODEL_OUTPUT_FILE, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "    print(\"\\nBaseline model pipeline complete. Model is saved and ready for prediction.\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce161d5b-0bb2-42e7-a9be-36b75ba900b1",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d420dd-79e4-4fe0-ab44-eec2f41cae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from training_dataset_baseline.csv...\n",
      "\n",
      "Detected 193 feature columns.\n",
      "\n",
      "Class Distribution:\n",
      "label\n",
      "0    0.739955\n",
      "1    0.260045\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training size: 1881\n",
      "Test size: 807\n",
      "\n",
      "Training Random Forest model...\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Wildlife       0.83      0.96      0.89       597\n",
      "    Wildlife       0.81      0.46      0.59       210\n",
      "\n",
      "    accuracy                           0.83       807\n",
      "   macro avg       0.82      0.71      0.74       807\n",
      "weighted avg       0.83      0.83      0.81       807\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[575  22]\n",
      " [114  96]]\n",
      "Confusion matrix plot saved to confusion_matrix.png\n",
      "\n",
      "Model saved to wildlife_model_rf.pkl\n",
      "Training pipeline complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "MODEL_OUTPUT_FILE = \"wildlife_model_rf.pkl\"\n",
    "PLOT_OUTPUT_FILE = \"confusion_matrix.png\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "    if not os.path.exists(DATASET_FILE):\n",
    "        print(f\"Error: Dataset file not found at {DATASET_FILE}\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv(DATASET_FILE)\n",
    "    df.fillna(0, inplace=True)  # Safety\n",
    "\n",
    "    # --- Identify label column ---\n",
    "    if \"label\" not in df.columns:\n",
    "        raise ValueError(\"ERROR: The dataset must include a 'label' column!\")\n",
    "\n",
    "    label_col = \"label\"\n",
    "    feature_cols = [col for col in df.columns if col not in [label_col, \"image_name\", \"cell_index\"]]\n",
    "\n",
    "    print(f\"\\nDetected {len(feature_cols)} feature columns.\")\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y = df[label_col]\n",
    "\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(y.value_counts(normalize=True))\n",
    "\n",
    "    # --- Train-test Split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining size: {len(X_train)}\")\n",
    "    print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "    # --- Model Training ---\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"No Wildlife\", \"Wildlife\"]))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "                yticklabels=[\"Act 0\", \"Act 1\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(PLOT_OUTPUT_FILE)\n",
    "    print(f\"Confusion matrix plot saved to {PLOT_OUTPUT_FILE}\")\n",
    "\n",
    "    # --- Save Model ---\n",
    "    with open(MODEL_OUTPUT_FILE, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"\\nModel saved to {MODEL_OUTPUT_FILE}\")\n",
    "    print(\"Training pipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "710b5088-d03b-4aef-a60a-04f61b4ad3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from training_dataset_baseline.csv...\n",
      "\n",
      "Detected 193 feature columns.\n",
      "\n",
      "Class Distribution:\n",
      "label\n",
      "0    0.739955\n",
      "1    0.260045\n",
      "Name: proportion, dtype: float64\n",
      "Training on 1881 samples, testing on 807 samples.\n",
      "\n",
      "Scaling data for parametric/non-parametric models...\n",
      "Data scaling complete.\n",
      "Imbalance ratio (0s/1s): 2.85\n",
      "\n",
      "--- Starting Model Comparison ---\n",
      "\n",
      "Training LogisticRegression (Parametric)...\n",
      "\n",
      "Training MLPClassifier (Neural Net)...\n",
      "\n",
      "Training XGBoost (Tree-based)...\n",
      "\n",
      "Training Random Forest (Tree-based)...\n",
      "\n",
      "Training KNeighborsClassifier (Non-Parametric)...\n",
      "\n",
      "Training SVC (Parametric)...\n",
      "\n",
      "--- Final Model Comparison Results ---\n",
      "                                       Precision (for 1)  Recall (for 1)  \\\n",
      "Model                                                                      \n",
      "XGBoost (Tree-based)                            0.758242        0.657143   \n",
      "SVC (Parametric)                                0.690476        0.690476   \n",
      "MLPClassifier (Neural Net)                      0.785714        0.576190   \n",
      "LogisticRegression (Parametric)                 0.575290        0.709524   \n",
      "Random Forest (Tree-based)                      0.813559        0.457143   \n",
      "KNeighborsClassifier (Non-Parametric)           0.702703        0.247619   \n",
      "\n",
      "                                       F1-Score (for 1)  Train Time (sec)  \n",
      "Model                                                                      \n",
      "XGBoost (Tree-based)                           0.704082          0.838999  \n",
      "SVC (Parametric)                               0.690476          0.517483  \n",
      "MLPClassifier (Neural Net)                     0.664835          0.759063  \n",
      "LogisticRegression (Parametric)                0.635394          0.151584  \n",
      "Random Forest (Tree-based)                     0.585366          0.367120  \n",
      "KNeighborsClassifier (Non-Parametric)          0.366197          0.067948  \n",
      "\n",
      "Comparison report saved to model_comparison_report.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import All the Models ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "df = pd.read_csv(DATASET_FILE)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- Identify label column ---\n",
    "if \"label\" not in df.columns:\n",
    "    raise ValueError(\"ERROR: The dataset must include a 'label' column!\")\n",
    "\n",
    "label_col = \"label\"\n",
    "feature_cols = [col for col in df.columns if col not in ['image_name', 'cell_index', 'label']]\n",
    "\n",
    "print(f\"\\nDetected {len(feature_cols)} feature columns.\")\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[label_col]\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# --- Train-test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
    "\n",
    "# --- 3. Create Scaled Data (CRITICAL STEP) ---\n",
    "print(\"\\nScaling data for parametric/non-parametric models...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "# --- 4. Handle Class Imbalance (for XGBoost) ---\n",
    "count_0 = y_train.value_counts()[0]\n",
    "count_1 = y_train.value_counts()[1]\n",
    "scale_pos_weight = count_0 / count_1\n",
    "print(f\"Imbalance ratio (0s/1s): {scale_pos_weight:.2f}\")\n",
    "\n",
    "# --- 5. Define Models ---\n",
    "# Note: \"Linear Regression\" and \"SVR\" are for regression.\n",
    "# Their classification counterparts are \"LogisticRegression\" and \"SVC\".\n",
    "models = {\n",
    "    \"LogisticRegression (Parametric)\": LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    \n",
    "    \"MLPClassifier (Neural Net)\": MLPClassifier(hidden_layer_sizes=(100, 50), # 2 hidden layers\n",
    "                                                max_iter=300, \n",
    "                                                random_state=42, \n",
    "                                                early_stopping=True),\n",
    "                                                \n",
    "    \"XGBoost (Tree-based)\": XGBClassifier(scale_pos_weight=scale_pos_weight, \n",
    "                                          random_state=42, \n",
    "                                          n_jobs=-1,\n",
    "                                          eval_metric='logloss'),\n",
    "\n",
    "    \"Random Forest (Tree-based)\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "\n",
    "    # --- WARNING: SLOW MODELS ---\n",
    "    # The models below (KNN, SVC) are EXTREMELY slow on large,\n",
    "    # high-dimensional data. Run them at your own risk.\n",
    "    # I have commented them out so you don't accidentally run them.\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \"KNeighborsClassifier (Non-Parametric)\": KNeighborsClassifier(n_neighbors=7, n_jobs=-1),\n",
    "    \n",
    "    \"SVC (Parametric)\": SVC(class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "# Define which models need scaled data\n",
    "models_that_need_scaling = [\"LogisticRegression (Parametric)\", \n",
    "                            \"MLPClassifier (Neural Net)\", \n",
    "                            \"KNeighborsClassifier (Non-Parametric)\", \n",
    "                            \"SVC (Parametric)\"]\n",
    "\n",
    "# --- 6. Train and Evaluate ---\n",
    "results = []\n",
    "print(\"\\n--- Starting Model Comparison ---\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Choose the correct dataset (scaled or unscaled)\n",
    "    if model_name in models_that_need_scaling:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        # Tree models (XGBoost, RandomForest) use unscaled data\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Get the classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Extract scores for the \"Wildlife (1)\" class\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1_score = report['1']['f1-score']\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision (for 1)\": precision,\n",
    "        \"Recall (for 1)\": recall,\n",
    "        \"F1-Score (for 1)\": f1_score,\n",
    "        \"Train Time (sec)\": end_time - start_time\n",
    "    })\n",
    "\n",
    "# --- 7. Print Final Comparison Table ---\n",
    "print(\"\\n--- Final Model Comparison Results ---\")\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1-Score (for 1)\", ascending=False)\n",
    "results_df = results_df.set_index(\"Model\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to a file for your presentation\n",
    "results_df.to_csv(\"model_comparison_report.csv\")\n",
    "print(\"\\nComparison report saved to model_comparison_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6d44395-fcc0-4e5f-a2a6-1059f07f7d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from training_dataset_baseline.csv...\n",
      "Data loaded and cleaned (NaNs filled with 0).\n",
      "Index(['B_mean', 'G_mean', 'R_mean', 'B_std', 'G_std', 'R_std', 'Edge_density',\n",
      "       'LBP_bin_0', 'LBP_bin_1', 'LBP_bin_2',\n",
      "       ...\n",
      "       'hog_89', 'hog_90', 'hog_91', 'hog_92', 'hog_93', 'hog_94', 'hog_95',\n",
      "       'label', 'image_name', 'cell_index'],\n",
      "      dtype='object', length=196)\n",
      "Detected 193 feature columns.\n",
      "Splitting data by image...\n",
      "Training on 2150 samples, testing on 538 samples.\n",
      "Training Stage 1 Model (XGBClassifier) on 193 features...\n",
      "Stage 1 training complete.\n",
      "Stage 1 model saved to stage1_model.pkl\n",
      "Best threshold: 0.28 F1: 0.7457627118644068\n",
      "Saved Stage-1 feature matrices (with stage1_prob).\n",
      "\n",
      "--- Stage 1 Model Evaluation ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "No Wildlife (0)       0.92      0.90      0.91       406\n",
      "   Wildlife (1)       0.71      0.75      0.73       132\n",
      "\n",
      "       accuracy                           0.86       538\n",
      "      macro avg       0.81      0.82      0.82       538\n",
      "   weighted avg       0.87      0.86      0.86       538\n",
      "\n",
      "\n",
      "Generating Stage 2 (context-aware) features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context features: 100%|█████████████████████████████████████████████████████████████████| 42/42 [00:00<00:00, 298.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Stage 2 Model (RandomForest) on 9 features...\n",
      "Stage 2 training complete.\n",
      "Stage 2 model saved to stage2_model.pkl\n",
      "\n",
      "--- Stage 2 Model Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context features: 100%|█████████████████████████████████████████████████████████████████| 42/42 [00:00<00:00, 354.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "No Wildlife (0)       0.99      0.98      0.99      2556\n",
      "   Wildlife (1)       0.71      0.75      0.73       132\n",
      "\n",
      "       accuracy                           0.97      2688\n",
      "      macro avg       0.85      0.87      0.86      2688\n",
      "   weighted avg       0.97      0.97      0.97      2688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All tasks complete. Both models and all plots/reports are generated.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "STAGE1_MODEL_FILE = \"stage1_model.pkl\" # Hand-crafted feature model\n",
    "STAGE2_MODEL_FILE = \"stage2_model.pkl\" # Context-aware model\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "df = pd.read_csv(DATASET_FILE)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#df.dropna(inplace=True)\n",
    "# This ensures we keep all 64 cells for every image.\n",
    "df.fillna(0, inplace=True)\n",
    "print(\"Data loaded and cleaned (NaNs filled with 0).\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Get all unique feature names\n",
    "feature_names = [col for col in df.columns if col not in ['image_name', 'cell_index', 'label']]\n",
    "print(f\"Detected {len(feature_names)} feature columns.\")\n",
    "if 'Edge_density' not in feature_names:\n",
    "    print(\"FATAL ERROR: 'Edge_density' feature not found, which is needed for Stage 2.\")\n",
    "    # This is a safety check\n",
    "\n",
    "\"\"\"# --- Identify label column ---\n",
    "if \"label\" not in df.columns:\n",
    "    raise ValueError(\"ERROR: The dataset must include a 'label' column!\")\n",
    "\n",
    "label_col = \"label\"\n",
    "[col for col in df.columns if col not in ['image_name', 'cell_index', 'label']]\n",
    "\n",
    "print(f\"\\nDetected {len(feature_cols)} feature columns.\")\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[label_col]\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# --- Train-test Split ---\n",
    "X_train_stage1, X_test_stage1, y_train_stage1, y_test_stage1 = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\"\"\"\n",
    "\n",
    "print(\"Splitting data by image...\")\n",
    "image_names = df['image_name'].unique()\n",
    "train_names, test_names = train_test_split(image_names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert back into DataFrames so they contain image_name + cell_index + features\n",
    "train_df = df.loc[X_train_stage1.index].copy()\n",
    "test_df  = df.loc[X_test_stage1.index].copy()\n",
    "\n",
    "# Prepare Stage 1 training and test sets\n",
    "X_train_stage1 = train_df[feature_names]\n",
    "y_train_stage1 = train_df['label']\n",
    "X_test_stage1 = test_df[feature_names]\n",
    "y_test_stage1 = test_df['label']\n",
    "print(f\"Training on {len(X_train_stage1)} samples, testing on {len(X_test_stage1)} samples.\")\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_stage1, y_train_stage1)\n",
    "\n",
    "# --- 3. Train Stage 1 Model (Hand-crafted Features) ---\n",
    "print(f\"Training Stage 1 Model (XGBClassifier) on {len(feature_names)} features...\")\n",
    "#model_stage1 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "model_stage1 = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, n_jobs=-1,eval_metric='logloss')\n",
    "model_stage1.fit(X_res, y_res)\n",
    "print(\"Stage 1 training complete.\")\n",
    "\n",
    "# Save Stage 1 model\n",
    "with open(STAGE1_MODEL_FILE, 'wb') as f:\n",
    "    pickle.dump(model_stage1, f)\n",
    "print(f\"Stage 1 model saved to {STAGE1_MODEL_FILE}\")\n",
    "\n",
    "train_df[\"stage1_prob\"] = model_stage1.predict_proba(train_df[feature_names])[:,1]\n",
    "test_df[\"stage1_prob\"]  = model_stage1.predict_proba(test_df[feature_names])[:,1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_t = 0.5\n",
    "\n",
    "for t in [i/100 for i in range(5, 50)]:\n",
    "    preds = (test_df[\"stage1_prob\"] > t).astype(int)\n",
    "    f = f1_score(y_test_stage1, preds)\n",
    "    if f > best_f1:\n",
    "        best_f1 = f\n",
    "        best_t = t\n",
    "\n",
    "print(\"Best threshold:\", best_t, \"F1:\", best_f1)\n",
    "\n",
    "# Save extracted features to CSV\n",
    "train_df.to_csv(\"stage1_train_features.csv\", index=False)\n",
    "test_df.to_csv(\"stage1_test_features.csv\", index=False)\n",
    "\n",
    "print(\"Saved Stage-1 feature matrices (with stage1_prob).\")\n",
    "# --- 4. Evaluate Stage 1 Model ---\n",
    "print(\"\\n--- Stage 1 Model Evaluation ---\")\n",
    "y_pred_stage1 = model_stage1.predict(X_test_stage1)\n",
    "report_stage1 = classification_report(y_test_stage1, y_pred_stage1, target_names=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "print(report_stage1)\n",
    "with open(\"classification_report.txt\", \"w\") as f:\n",
    "    f.write(\"--- STAGE 1 (Hand-crafted Features Only) ---\\n\")\n",
    "    f.write(report_stage1)\n",
    "\n",
    "cm_stage1 = confusion_matrix(y_test_stage1, y_pred_stage1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_stage1, annot=True, fmt='d', cmap='Blues', xticklabels=['No Wildlife (0)', 'Wildlife (1)'], yticklabels=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "plt.title('Stage 1 (Hand-crafted) Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(\"confusion_matrix_stage1.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 5. Create Stage 2 Dataset (Context-Aware) ---\n",
    "print(\"\\nGenerating Stage 2 (context-aware) features...\")\n",
    "\n",
    "def create_stage2_features(image_df, stage1_model):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame for one or more images, gets Stage 1 probabilities,\n",
    "    and builds the 9-feature (self + 8 neighbors) dataset.\n",
    "    \"\"\"\n",
    "    # Get Stage 1 probabilities (for class 1)\n",
    "    stage1_probs = stage1_model.predict_proba(image_df[feature_names])[:, 1]\n",
    "    image_df['stage1_prob'] = stage1_probs\n",
    "    \n",
    "    stage2_features = []\n",
    "    stage2_labels = []\n",
    "    \n",
    "    for image_name in tqdm(image_df['image_name'].unique(), desc=\"Building context features\"):\n",
    "        img_cells = image_df[image_df['image_name'] == image_name].sort_values('cell_index')\n",
    "        \n",
    "        # Create 8x8 grid of probabilities\n",
    "        prob_grid = img_cells.set_index('cell_index')['stage1_prob'].reindex(range(64), fill_value=0.0).values.reshape(8, 8)\n",
    "        label_grid = img_cells.set_index('cell_index')['label'].reindex(range(64), fill_value=0.0).values.reshape(8, 8)\n",
    "        edge_grid = img_cells.set_index('cell_index')['Edge_density'].reindex(range(64), fill_value=0.0).values.reshape(8, 8)\n",
    "        \n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                # 10 features: 9 probabilities + 1 edge density\n",
    "                features = np.zeros(10)\n",
    "                features[0] = prob_grid[r, c] # Self prob\n",
    "                \n",
    "                # Get neighbor probabilities, using 0.0 for padding at edges\n",
    "                for i, (dr, dc) in enumerate([(-1, -1), (-1, 0), (-1, 1),\n",
    "                                               ( 0, -1),           ( 0, 1),\n",
    "                                               ( 1, -1), ( 1, 0), ( 1, 1)]):\n",
    "                    nr, nc = r + dr, c + dc\n",
    "                    if 0 <= nr < 8 and 0 <= nc < 8:\n",
    "                        features[i+1] = prob_grid[nr, nc]\n",
    "                # --- Add the cell's own Edge_density as the 10th feature ---\n",
    "                features[9] = edge_grid[r, c]\n",
    "                stage2_features.append(features)\n",
    "                stage2_labels.append(label_grid[r, c])\n",
    "                \n",
    "    return np.array(stage2_features), np.array(stage2_labels)\n",
    "\n",
    "X_train_stage2, y_train_stage2 = create_stage2_features(train_df, model_stage1)\n",
    "\n",
    "# --- 6. Train Stage 2 Model (Context-Aware) ---\n",
    "print(f\"\\nTraining Stage 2 Model (RandomForest) on 9 features...\")\n",
    "# Using a simple model here is best. RandomForest would also work.\n",
    "# model_stage2 = LogisticRegression(random_state=42, class_weight='balanced') # <-- OLD MODEL\n",
    "model_stage2 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced') # <-- NEW, MORE POWERFUL MODEL\n",
    "model_stage2.fit(X_train_stage2, y_train_stage2)\n",
    "print(\"Stage 2 training complete.\")\n",
    "\n",
    "# Save Stage 2 model\n",
    "with open(STAGE2_MODEL_FILE, 'wb') as f:\n",
    "    pickle.dump(model_stage2, f)\n",
    "print(f\"Stage 2 model saved to {STAGE2_MODEL_FILE}\")\n",
    "\n",
    "# --- 7. Evaluate Stage 2 Model ---\n",
    "print(\"\\n--- Stage 2 Model Evaluation ---\")\n",
    "# Create test features\n",
    "X_test_stage2, y_test_stage2 = create_stage2_features(test_df, model_stage1)\n",
    "y_pred_stage2 = model_stage2.predict(X_test_stage2)\n",
    "\n",
    "report_stage2 = classification_report(y_test_stage2, y_pred_stage2, target_names=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "print(report_stage2)\n",
    "with open(\"classification_report.txt\", \"a\") as f: # Append to the report file\n",
    "    f.write(\"\\n\\n--- STAGE 2 (Context-Aware) ---\\n\")\n",
    "    f.write(report_stage2)\n",
    "\n",
    "cm_stage2 = confusion_matrix(y_test_stage2, y_pred_stage2)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_stage2, annot=True, fmt='d', cmap='Blues', xticklabels=['No Wildlife (0)', 'Wildlife (1)'], yticklabels=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "plt.title('Stage 2 (Context-Aware) Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(\"confusion_matrix_stage2.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll tasks complete. Both models and all plots/reports are generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69abe98-999f-462f-8b04-2c7516ffc2e7",
   "metadata": {},
   "source": [
    "EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09c1c726-64af-441b-b1a4-4d029900e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from stage1_model.pkl...\n",
      "Loading model from stage2_model.pkl...\n",
      "Output images will be saved to predicted_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████████████████████████████████████████████████████████| 279/279 [10:18<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving final predictions CSV...\n",
      "Prediction complete. View highlighted images in 'predicted_images' and results in 'predictions.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from skimage.feature import local_binary_pattern\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "STAGE1_MODEL_FILE = \"stage1_model.pkl\"         # Your saved .pkl model file\n",
    "STAGE2_MODEL_FILE = \"stage2_model.pkl\"         # Your saved .pkl model file\n",
    "INPUT_IMAGE_DIR = \"preprocessed_images\"     # Folder of new, unseen images\n",
    "OUTPUT_IMAGE_DIR = \"predicted_images\"     # Folder for highlighted images\n",
    "OUTPUT_CSV_FILE = \"predictions.csv\"       # The final CSV output\n",
    "\n",
    "# --- Grid & Preprocessing Constants ---\n",
    "TARGET_WIDTH = 800\n",
    "TARGET_HEIGHT = 600\n",
    "TARGET_ASPECT_RATIO = TARGET_WIDTH / TARGET_HEIGHT\n",
    "GRID_ROWS = 8\n",
    "GRID_COLS = 8\n",
    "CELL_HEIGHT = TARGET_HEIGHT // GRID_ROWS # 75\n",
    "CELL_WIDTH = TARGET_WIDTH // GRID_COLS   # 100\n",
    "\n",
    "\n",
    "# --- MAIN PREDICTION SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. Load Model ---\n",
    "    print(f\"Loading model from {STAGE1_MODEL_FILE}...\")\n",
    "    if not os.path.exists(STAGE1_MODEL_FILE):\n",
    "        print(f\"Error: Model file {STAGE1_MODEL_FILE} not found.\")\n",
    "        print(\"Please run model_trainer.py first.\")\n",
    "        exit()\n",
    "    print(f\"Loading model from {STAGE2_MODEL_FILE}...\")\n",
    "    if not os.path.exists(STAGE2_MODEL_FILE):\n",
    "        print(f\"Error: Model file {STAGE2_MODEL_FILE} not found.\")\n",
    "        print(\"Please run model_trainer.py first.\")\n",
    "        exit()\n",
    "    # Load Stage 1 & Stage 2 models\n",
    "    with open(STAGE1_MODEL_FILE,\"rb\") as f:\n",
    "        model_stage1 = pickle.load(f)\n",
    "    with open(STAGE2_MODEL_FILE,\"rb\") as f:\n",
    "        model_stage2 = pickle.load(f)\n",
    "\n",
    "\n",
    "    # --- 2. Setup Directories ---\n",
    "    if not os.path.exists(INPUT_IMAGE_DIR):\n",
    "        print(f\"Error: Input directory {INPUT_IMAGE_DIR} not found.\")\n",
    "        print(\"Please create it and add images you want to predict.\")\n",
    "        exit()\n",
    "        \n",
    "    os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "    print(f\"Output images will be saved to {OUTPUT_IMAGE_DIR}\")\n",
    "    \n",
    "    image_paths = glob.glob(os.path.join(INPUT_IMAGE_DIR, '*.jpg'))\n",
    "    image_paths += glob.glob(os.path.join(INPUT_IMAGE_DIR, '*.png'))\n",
    "    image_paths += glob.glob(os.path.join(INPUT_IMAGE_DIR, '*.jpeg'))\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"Error: No images found in {INPUT_IMAGE_DIR}.\")\n",
    "        exit()\n",
    "\n",
    "    prob_grid = np.zeros((GRID_ROWS, GRID_COLS))\n",
    "    # --- 3. Run Prediction Loop ---\n",
    "    all_csv_rows = []\n",
    "    \n",
    "    for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        image_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        raw_image = cv2.imread(image_path)\n",
    "        if raw_image is None:\n",
    "            print(f\"Warning: Skipping invalid image {image_name}\")\n",
    "            continue\n",
    "        \n",
    "        processed_image = preprocess_image(raw_image)\n",
    "        \n",
    "        # This copy is for drawing our highlights on\n",
    "        output_image = processed_image.copy()\n",
    "        \n",
    "        # This list will hold 'c01', 'c02', ... predictions\n",
    "        csv_row = [image_name]\n",
    "        \n",
    "        # Iterate over the 8x8 grid\n",
    "        all_cells_features = []\n",
    "        for i in range(GRID_ROWS):\n",
    "            for j in range(GRID_COLS):\n",
    "                # Extract the 100x75 cell\n",
    "                y1, y2 = i * CELL_HEIGHT, (i + 1) * CELL_HEIGHT\n",
    "                x1, x2 = j * CELL_WIDTH, (j + 1) * CELL_WIDTH\n",
    "                cell = processed_image[y1:y2, x1:x2]\n",
    "                \n",
    "                # --- CORE ML PIPELINE ---\n",
    "                # 1. Extract features\n",
    "                all_cells_features.append(extract_features_for_cell(cell))\n",
    "                \n",
    "        features_df = pd.DataFrame(all_cells_features, columns=FEATURE_NAMES)\n",
    "        stage1_probs = model_stage1.predict_proba(features_df)[:, 1]\n",
    "        prob_grid = stage1_probs.reshape(GRID_ROWS, GRID_COLS)\n",
    "        \n",
    "        # Now run Stage-2 context model\n",
    "        csv_row = [image_name]\n",
    "        pred_grid = np.zeros((GRID_ROWS, GRID_COLS), dtype=int)\n",
    "        \n",
    "        neighbors = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "        \n",
    "        for r in range(GRID_ROWS):\n",
    "            for c in range(GRID_COLS):\n",
    "                # Build the 9-feature vector (self + 8 neighbors)\n",
    "                stage2_features = np.zeros(9)\n",
    "                stage2_features[0] = prob_grid[r, c] # Self\n",
    "                for i, (dr, dc) in enumerate([(-1, -1), (-1, 0), (-1, 1),\n",
    "                                               ( 0, -1),           ( 0, 1),\n",
    "                                               ( 1, -1), ( 1, 0), ( 1, 1)]):\n",
    "                    nr, nc = r + dr, c + dc\n",
    "                    if 0 <= nr < 8 and 0 <= nc < 8:\n",
    "                        stage2_features[i+1] = prob_grid[nr, nc]\n",
    "                \n",
    "                # Get final prediction from Stage 2 model\n",
    "                final_prediction = model_stage2.predict(stage2_features.reshape(1, -1))[0]\n",
    "                csv_row.append(int(final_prediction))\n",
    "                \n",
    "                # --- 4. Visualize Prediction ---\n",
    "                if final_prediction == 1:\n",
    "                    x1, y1 = c * CELL_WIDTH, r * CELL_HEIGHT\n",
    "                    x2, y2 = (c + 1) * CELL_WIDTH, (r + 1) * CELL_HEIGHT\n",
    "                    overlay = output_image.copy()\n",
    "                    cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), -1)\n",
    "                    alpha = 0.4\n",
    "                    cv2.addWeighted(overlay, alpha, output_image, 1 - alpha, 0, output_image)\n",
    "                    \n",
    "        # Add this image's row of 64 predictions to our list\n",
    "        all_csv_rows.append(csv_row)\n",
    "        \n",
    "        # Save the highlighted image\n",
    "        output_path = os.path.join(OUTPUT_IMAGE_DIR, f\"pred_{image_name}\")\n",
    "        cv2.imwrite(output_path, output_image)\n",
    "\n",
    "    # --- 4. Save Final CSV Output ---\n",
    "    print(\"\\nSaving final predictions CSV...\")\n",
    "    \n",
    "    # Create column headers: 'image_name', 'c01', 'c02', ..., 'c64'\n",
    "    headers = ['image_name'] + [f'c{i:02d}' for i in range(1, (GRID_ROWS * GRID_COLS) + 1)]\n",
    "    \n",
    "    # Create and save the DataFrame\n",
    "    csv_df = pd.DataFrame(all_csv_rows, columns=headers)\n",
    "    csv_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "    \n",
    "    print(f\"Prediction complete. View highlighted images in '{OUTPUT_IMAGE_DIR}' and results in '{OUTPUT_CSV_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43f3e5-2c94-4ebb-942d-8faa6ebbcda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
