{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5de2996-808a-44f3-8921-4b0c8e343189",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33466722-8875-4daa-ad25-b1f77231a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   1%|▋                                                                         | 4/465 [00:01<02:43,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image 141120142185.jpg. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  11%|████████▏                                                                | 52/465 [00:16<02:09,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image CIMG0079~3.JPG. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|████████████████████████████████████████████████████████████████████████| 465/465 [01:46<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image CIMG0579(2).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0579.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0580.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0584.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0585.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0593.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0594.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0595.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0599.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0601.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0602.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0604.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0605.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0608.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0642.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0672.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0757(1).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0757(2).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0757.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0914(1).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0914.jpg. Skipping.\n",
      "Warning: Could not load image CIMG0958.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0975.JPG. Skipping.\n",
      "Warning: Could not load image CIMG0979.jpg. Skipping.\n",
      "Warning: Could not load image CIMG0980(1).JPG. Skipping.\n",
      "Warning: Could not load image CIMG0980.jpg. Skipping.\n",
      "Warning: Could not load image CIMG1147.JPG. Skipping.\n",
      "Warning: Could not load image CIMG1270.JPG. Skipping.\n",
      "Warning: Could not load image DSC00095.JPG. Skipping.\n",
      "Warning: Could not load image DSC00096.JPG. Skipping.\n",
      "Warning: Could not load image DSC00182.JPG. Skipping.\n",
      "Warning: Could not load image DSC00183.JPG. Skipping.\n",
      "Warning: Could not load image DSC00186.JPG. Skipping.\n",
      "Warning: Could not load image DSC00319.JPG. Skipping.\n",
      "Warning: Could not load image DSC00320.JPG. Skipping.\n",
      "Warning: Could not load image DSC00322.JPG. Skipping.\n",
      "Warning: Could not load image DSC00342.JPG. Skipping.\n",
      "Warning: Could not load image DSC00343.JPG. Skipping.\n",
      "Warning: Could not load image DSC00344.JPG. Skipping.\n",
      "Warning: Could not load image DSC00386.JPG. Skipping.\n",
      "Warning: Could not load image DSC02847.JPG. Skipping.\n",
      "Warning: Could not load image DSC02848.JPG. Skipping.\n",
      "Warning: Could not load image DSC_0359.JPG. Skipping.\n",
      "Warning: Could not load image DSC_0360.JPG. Skipping.\n",
      "Warning: Could not load image IMG_1724.JPG. Skipping.\n",
      "Warning: Could not load image IMG_2510.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3332(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3332.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3334(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3334.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3337.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3338.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3347.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3348.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3361.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3368.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3369(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3369.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3370.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3371.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3372.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3373.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3378.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3385.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3386.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3387.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3388.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3393.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3394.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3395.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3403(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3403.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3404.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3406.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3409.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3410(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3410.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3411.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3412.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3416.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3417.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3418.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3419.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3438.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3439.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3477.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3550.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3569(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_3637.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3653.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3674.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3676.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3682.JPG. Skipping.\n",
      "Warning: Could not load image IMG-20140317-WA0001.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180401_143225122_HDR.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180724_170028451.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180724_170036332.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180724_170112733.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154742802.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154804552.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154917733_BURST000_COVER_TOP.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154940261_BURST000_COVER_TOP.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20180726_154949063_BURST001.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20211223_163150170_HDR.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20221207_075613681.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20240822_085549698~3.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250524_140427028.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250524_143429630.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250627_104711559_HDR.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250627_105023208.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_121136381.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_121136381~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_125343397.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_125546309.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_142522.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250705_170544521.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250708_120702744_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250709_163052484_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250709_164031681_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250709_164054571_HDR~2.jpg. Skipping.\n",
      "Warning: Could not load image IMG_20250710_100837.jpg. Skipping.\n",
      "Warning: Could not load image IMG_3686.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3687.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3699.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3700.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3701.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3706.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3747.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3748.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3761.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3839.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3844.JPG. Skipping.\n",
      "Warning: Could not load image IMG_3858.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4131.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4142.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4150.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4158(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_4158.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4164.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4165.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4171.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4172.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4173.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4174.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4175.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4176.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4182(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_4184.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4185.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4186.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4190.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4191.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4192.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4193.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4194(1).JPG. Skipping.\n",
      "Warning: Could not load image IMG_4194.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4195.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4196.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4197.JPG. Skipping.\n",
      "Warning: Could not load image IMG_4198.JPG. Skipping.\n",
      "Warning: Could not load image Photo0044.jpg. Skipping.\n",
      "Warning: Could not load image RIMG0047.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0050.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0051.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0066.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0075.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0076.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0079.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0081.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0124.JPG. Skipping.\n",
      "Warning: Could not load image RIMG0919.JPG. Skipping.\n",
      "Warning: Could not load image RIMG1328.JPG. Skipping.\n",
      "Warning: Could not load image RIMG1945.JPG. Skipping.\n",
      "Warning: Could not load image Rimg0149s.jpg. Skipping.\n",
      "Warning: Could not load image p10.jpg. Skipping.\n",
      "Warning: Could not load image p12.jpg. Skipping.\n",
      "Warning: Could not load image p16.jpg. Skipping.\n",
      "Warning: Could not load image p17.jpg. Skipping.\n",
      "Warning: Could not load image p19.jpg. Skipping.\n",
      "Warning: Could not load image p24.jpg. Skipping.\n",
      "Warning: Could not load image p25.jpg. Skipping.\n",
      "Warning: Could not load image p4.jpg. Skipping.\n",
      "Warning: Could not load image p6.jpg. Skipping.\n",
      "Warning: Could not load image p7.jpg. Skipping.\n",
      "Warning: Could not load image p8.jpg. Skipping.\n",
      "Warning: Could not load image p9.jpg. Skipping.\n",
      "\n",
      "Feature extraction complete. Assembling final dataset...\n",
      "Success! Training dataset saved to training_dataset_baseline.csv\n",
      "Total rows (cells): 17728\n",
      "Total features: 193\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from tqdm import tqdm # A progress bar! pip install tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PREPROCESSED_DIR = \"preprocessed_images\"\n",
    "LABELS_FILE = \"label_results_merged (1).csv\" # Your manually created labels\n",
    "OUTPUT_DATASET = \"training_dataset_baseline.csv\" # The file this script will create\n",
    "\n",
    "GRID_ROWS = 8\n",
    "GRID_COLS = 8\n",
    "CELL_HEIGHT = 600 // GRID_ROWS # 75 pixels\n",
    "CELL_WIDTH = 800 // GRID_COLS  # 100 pixels\n",
    "\n",
    "# LBP (Local Binary Patterns) configuration\n",
    "LBP_POINTS = 24 # Number of points to check around a pixel\n",
    "LBP_RADIUS = 3  # Radius of the circle\n",
    "\n",
    "# --- Color Histogram configuration ---\n",
    "HIST_BINS = [4, 4, 4] # 4 bins for B, 4 for G, 4 for R = 4*4*4=64 features\n",
    "\n",
    "# --- HOG Configuration ---\n",
    "HOG_ORIENTATIONS = 8\n",
    "HOG_PIXELS_PER_CELL = (25, 25) # 100x75 cell -> (100/25)x(75/25) = 4x3 grid of cells\n",
    "HOG_CELLS_PER_BLOCK = (1, 1)   # No block normalization\n",
    "# Total HOG Features = 4 * 3 * 1 * 1 * 8 = 96 features\n",
    "N_HOG_FEATURES = (CELL_WIDTH // HOG_PIXELS_PER_CELL[0]) * \\\n",
    "                 (CELL_HEIGHT // HOG_PIXELS_PER_CELL[1]) * \\\n",
    "                 (HOG_CELLS_PER_BLOCK[0] * HOG_CELLS_PER_BLOCK[1]) * \\\n",
    "                 HOG_ORIENTATIONS\n",
    "\n",
    "def extract_features_for_cell(cell):\n",
    "    \"\"\"\n",
    "    This function takes one 100x75 cell (a BGR image) and returns\n",
    "    a 1D numpy array of its calculated baseline features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    # --- 1. Color Features (Average & Std Dev BGR) ---\n",
    "    # We now capture both mean and standard deviation\n",
    "    (means, stds) = cv2.meanStdDev(cell)\n",
    "    features.extend(means.flatten()) # Add B_mean, G_mean, R_mean\n",
    "    features.extend(stds.flatten()) # Add B_std, G_std, R_std\n",
    "\n",
    "    # --- 2. Grayscale & Edge Features ---\n",
    "    gray_cell = cv2.cvtColor(cell, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply CLAHE to enhance local contrast and fight camouflage\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
    "    gray_cell = clahe.apply(gray_cell)\n",
    "    \n",
    "    # Canny Edge Density\n",
    "    # We still need grayscale stats to find good Canny thresholds\n",
    "    (gray_mean, gray_std) = cv2.meanStdDev(gray_cell)\n",
    "    sigma = gray_std[0][0]\n",
    "    v = gray_mean[0][0]\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    \n",
    "    edges = cv2.Canny(gray_cell, lower, upper)\n",
    "    edge_density = np.sum(edges > 0) / (CELL_WIDTH * CELL_HEIGHT)\n",
    "    features.append(edge_density)\n",
    "\n",
    "    # --- 3. Texture Features (LBP) ---\n",
    "    # We use the 'uniform' method, which is robust and limits features.\n",
    "    # It results in (LBP_POINTS + 2) features.\n",
    "    lbp = local_binary_pattern(gray_cell, LBP_POINTS, LBP_RADIUS, method=\"uniform\")\n",
    "    \n",
    "    # Create a normalized histogram of the LBP results\n",
    "    (hist, _) = np.histogram(lbp.ravel(),\n",
    "                             bins=np.arange(0, LBP_POINTS + 3),\n",
    "                             range=(0, LBP_POINTS + 2))\n",
    "    \n",
    "    # Normalize histogram to be a probability distribution\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6) # Add epsilon to avoid division by zero\n",
    "    \n",
    "    features.extend(hist)\n",
    "\n",
    "    # --- 4. 3D Color Histogram ---\n",
    "    # Calculate a 3D histogram for B, G, R channels\n",
    "    hist_3d = cv2.calcHist([cell], [0, 1, 2], None, HIST_BINS,\n",
    "                           [0, 256, 0, 256, 0, 256])\n",
    "    \n",
    "    # Normalize the histogram\n",
    "    cv2.normalize(hist_3d, hist_3d, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "    \n",
    "    #Flatten the 3D histogram into a 1D vector\n",
    "    features.extend(hist_3d.flatten())\n",
    "\n",
    "    # --- 5. HOG (Histogram of Oriented Gradients) Features ---\n",
    "    hog_features = hog(gray_cell, orientations=HOG_ORIENTATIONS,\n",
    "                       pixels_per_cell=HOG_PIXELS_PER_CELL,\n",
    "                       cells_per_block=HOG_CELLS_PER_BLOCK,\n",
    "                       visualize=False, block_norm='L1')\n",
    "    features.extend(hog_features.flatten())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting feature extraction...\")\n",
    "    \n",
    "    # Load the labels\n",
    "    if not os.path.exists(LABELS_FILE):\n",
    "        print(f\"Error: Labels file not found at {LABELS_FILE}\")\n",
    "        exit()\n",
    "    labels_df = pd.read_csv(LABELS_FILE)\n",
    "    labels_df.rename(columns={\"filename\": \"image_name\"}, inplace=True)\n",
    "    new_columns = {\"cell_\" + str(i): f\"c{i+1:02d}\" for i in range(64)}\n",
    "    labels_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "    cell_indices = []\n",
    "\n",
    "    # Use tqdm for a progress bar. Iterating rows is like iterating images.\n",
    "    for index, row in tqdm(labels_df.iterrows(), total=labels_df.shape[0], desc=\"Processing Images\"):\n",
    "        image_name = row['image_name']\n",
    "        image_path = os.path.join(PREPROCESSED_DIR, image_name)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not load image {image_name}. Skipping.\")\n",
    "            continue\n",
    "        l = [image_name]*GRID_ROWS*GRID_COLS\n",
    "        all_images+=l\n",
    "            \n",
    "        # Iterate over the 8x8 grid\n",
    "        cell_index = 1\n",
    "        for i in range(GRID_ROWS):\n",
    "            for j in range(GRID_COLS):\n",
    "                # Calculate coordinates for the cell\n",
    "                y1, y2 = i * CELL_HEIGHT, (i + 1) * CELL_HEIGHT\n",
    "                x1, x2 = j * CELL_WIDTH, (j + 1) * CELL_WIDTH\n",
    "                \n",
    "                # Extract the 100x75 cell\n",
    "                cell = image[y1:y2, x1:x2]\n",
    "                \n",
    "                # Get the label for this cell from the CSV\n",
    "                # Column name is 'c01', 'c02', ... 'c64'\n",
    "                cell_label_col = f'c{(i * GRID_COLS + j + 1):02d}'\n",
    "                label = row[cell_label_col]\n",
    "                \n",
    "                # --- This is the core step ---\n",
    "                features = extract_features_for_cell(cell)\n",
    "                \n",
    "                all_features.append(features)\n",
    "                all_labels.append(label)\n",
    "                cell_indices.append(cell_index)\n",
    "                cell_index+=1\n",
    "\n",
    "    print(\"\\nFeature extraction complete. Assembling final dataset...\")\n",
    "\n",
    "    # Create a list of feature names for the CSV header\n",
    "    # This list is now updated to match our baseline features\n",
    "    feature_names = ['B_mean', 'G_mean', 'R_mean', 'B_std', 'G_std', 'R_std', 'Edge_density']\n",
    "    # Add LBP histogram feature names\n",
    "    feature_names += [f'LBP_bin_{k}' for k in range(LBP_POINTS + 2)]\n",
    "    # Add Color Histogram names (e.g., 'hist_B0_G0_R0')\n",
    "    hist_feature_names = [f'hist_B{b}_G{g}_R{r}' \n",
    "                          for b in range(HIST_BINS[0]) \n",
    "                          for g in range(HIST_BINS[1]) \n",
    "                          for r in range(HIST_BINS[2])]\n",
    "    feature_names.extend(hist_feature_names)\n",
    "    # Add HOG feature names\n",
    "    hog_feature_names = [f'hog_{i}' for i in range(N_HOG_FEATURES)]\n",
    "    feature_names.extend(hog_feature_names)\n",
    "\n",
    "    # Create the final DataFrame\n",
    "    X = np.array(all_features)\n",
    "    y = np.array(all_labels)\n",
    "    image_names = np.array(all_images)\n",
    "    cell_indices = np.array(cell_indices)\n",
    "\n",
    "    dataset_df = pd.DataFrame(X, columns=feature_names)\n",
    "    dataset_df['label'] = y\n",
    "    dataset_df['image_name'] = image_names\n",
    "    dataset_df['cell_index'] = cell_indices\n",
    "    \n",
    "    # Save to CSV\n",
    "    dataset_df.to_csv(OUTPUT_DATASET, index=False)\n",
    "    \n",
    "    print(f\"Success! Training dataset saved to {OUTPUT_DATASET}\")\n",
    "    print(f\"Total rows (cells): {len(dataset_df)}\")\n",
    "    print(f\"Total features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db323229-7348-4adb-afea-d9188ad1d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mithr\\AppData\\Local\\Temp\\ipykernel_7472\\4124604336.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0s: 20926\n",
      "Count of 1s: 8369\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for _, row in labels_df.iterrows():  \n",
    "    for i in range(1,64):\n",
    "        label = row[i]\n",
    "        labels.append(label)\n",
    "count_zeros = labels.count(0)\n",
    "count_ones = labels.count(1)\n",
    "print(f\"Count of 0s: {count_zeros}\")\n",
    "print(f\"Count of 1s: {count_ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84deb11b-e7a7-4f54-959d-f6aaf8508932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport pickle\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport os\\n\\n# --- CONFIGURATION ---\\nDATASET_FILE = \"training_dataset_baseline.csv\"\\nMODEL_OUTPUT_FILE = \"wildlife_model.pkl\" # The final, saved model\\nPLOT_OUTPUT_FILE = \"confusion_matrix.png\" # The evaluation plot\\n\\n# --- MAIN SCRIPT ---\\nif __name__ == \"__main__\":\\n\\n    # --- 1. Load Data ---\\n    print(f\"Loading dataset from {DATASET_FILE}...\")\\n    if not os.path.exists(DATASET_FILE):\\n        print(f\"Error: Dataset file not found at {DATASET_FILE}\")\\n        print(\"Please run feature_extractor.py first.\")\\n        exit()\\n\\n    df = pd.read_csv(DATASET_FILE)\\n\\n    # Simple data cleaning: Fill any potential NaN values with 0\\n    # (This can happen with LBP on all-black cells, etc.)\\n    df.fillna(0, inplace=True)\\n\\n    # --- 2. Split Data (Features & Labels) ---\\n    print(\"Splitting data into training and test sets...\")\\n\\n    X = df.drop(\"label\", axis=1) # All columns EXCEPT \\'label\\'\\n    y = df[\"label\"]             # Just the \\'label\\' column\\n\\n    # Check for class imbalance (important for your report)\\n    print(\"\\nClass Distribution:\")\\n    print(y.value_counts(normalize=True))\\n\\n    # Split the data. \\n    # test_size=0.3 means 30% for testing, 70% for training.\\n    # stratify=y is CRITICAL: it ensures your train and test sets have\\n    # the same percentage of 0s and 1s as the full dataset.\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \\n                                                        test_size=0.3, \\n                                                        random_state=42, \\n                                                        stratify=y)\\n\\n    print(f\"\\nTraining set size: {len(X_train)} cells\")\\n    print(f\"Test set size: {len(X_test)} cells\")\\n\\n    # --- 3. Train Baseline Model ---\\n    print(\"\\nTraining baseline RandomForestClassifier...\")\\n\\n    # n_estimators=100 is a good default (100 \"trees\" in the forest)\\n    # n_jobs=-1 uses all your CPU cores to speed up training\\n    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\\n\\n    # This is the step that does the actual learning\\n    model.fit(X_train, y_train)\\n\\n    print(\"Model training complete.\")\\n\\n    # --- 4. Evaluate Model ---\\n    print(\"\\nEvaluating model on the test set...\")\\n\\n    # Get predictions for the test data\\n    y_pred = model.predict(X_test)\\n\\n    # Print a full report\\n    print(\"\\n--- Classification Report ---\")\\n    print(classification_report(y_test, y_pred, target_names=[\"No Wildlife (0)\", \"Wildlife (1)\"]))\\n\\n    # Generate and plot a confusion matrix\\n    print(\"\\n--- Confusion Matrix ---\")\\n    cm = confusion_matrix(y_test, y_pred)\\n    print(cm)\\n\\n    # Create a heatmap plot of the confusion matrix\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(cm, annot=True, fmt=\\'d\\', cmap=\\'Blues\\', \\n                xticklabels=[\"Predicted 0\", \"Predicted 1\"], \\n                yticklabels=[\"Actual 0\", \"Actual 1\"])\\n    plt.title(\\'Confusion Matrix\\')\\n    plt.ylabel(\\'Actual Label\\')\\n    plt.xlabel(\\'Predicted Label\\')\\n    plt.savefig(PLOT_OUTPUT_FILE)\\n    print(f\"\\nConfusion matrix plot saved to {PLOT_OUTPUT_FILE}\")\\n\\n    # --- 5. Save the Final Model ---\\n    print(f\"\\nSaving trained model to {MODEL_OUTPUT_FILE}...\")\\n\\n    # Use pickle to save the model to a file\\n    with open(MODEL_OUTPUT_FILE, \"wb\") as f:\\n        pickle.dump(model, f)\\n\\n    print(\"\\nBaseline model pipeline complete. Model is saved and ready for prediction.\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "MODEL_OUTPUT_FILE = \"wildlife_model.pkl\" # The final, saved model\n",
    "PLOT_OUTPUT_FILE = \"confusion_matrix.png\" # The evaluation plot\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "    if not os.path.exists(DATASET_FILE):\n",
    "        print(f\"Error: Dataset file not found at {DATASET_FILE}\")\n",
    "        print(\"Please run feature_extractor.py first.\")\n",
    "        exit()\n",
    "        \n",
    "    df = pd.read_csv(DATASET_FILE)\n",
    "\n",
    "    # Simple data cleaning: Fill any potential NaN values with 0\n",
    "    # (This can happen with LBP on all-black cells, etc.)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # --- 2. Split Data (Features & Labels) ---\n",
    "    print(\"Splitting data into training and test sets...\")\n",
    "    \n",
    "    X = df.drop(\"label\", axis=1) # All columns EXCEPT 'label'\n",
    "    y = df[\"label\"]             # Just the 'label' column\n",
    "\n",
    "    # Check for class imbalance (important for your report)\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(y.value_counts(normalize=True))\n",
    "    \n",
    "    # Split the data. \n",
    "    # test_size=0.3 means 30% for testing, 70% for training.\n",
    "    # stratify=y is CRITICAL: it ensures your train and test sets have\n",
    "    # the same percentage of 0s and 1s as the full dataset.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.3, \n",
    "                                                        random_state=42, \n",
    "                                                        stratify=y)\n",
    "    \n",
    "    print(f\"\\nTraining set size: {len(X_train)} cells\")\n",
    "    print(f\"Test set size: {len(X_test)} cells\")\n",
    "\n",
    "    # --- 3. Train Baseline Model ---\n",
    "    print(\"\\nTraining baseline RandomForestClassifier...\")\n",
    "    \n",
    "    # n_estimators=100 is a good default (100 \"trees\" in the forest)\n",
    "    # n_jobs=-1 uses all your CPU cores to speed up training\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # This is the step that does the actual learning\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- 4. Evaluate Model ---\n",
    "    print(\"\\nEvaluating model on the test set...\")\n",
    "    \n",
    "    # Get predictions for the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print a full report\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"No Wildlife (0)\", \"Wildlife (1)\"]))\n",
    "    \n",
    "    # Generate and plot a confusion matrix\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Create a heatmap plot of the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[\"Predicted 0\", \"Predicted 1\"], \n",
    "                yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(PLOT_OUTPUT_FILE)\n",
    "    print(f\"\\nConfusion matrix plot saved to {PLOT_OUTPUT_FILE}\")\n",
    "\n",
    "    # --- 5. Save the Final Model ---\n",
    "    print(f\"\\nSaving trained model to {MODEL_OUTPUT_FILE}...\")\n",
    "    \n",
    "    # Use pickle to save the model to a file\n",
    "    with open(MODEL_OUTPUT_FILE, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "    print(\"\\nBaseline model pipeline complete. Model is saved and ready for prediction.\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce161d5b-0bb2-42e7-a9be-36b75ba900b1",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d420dd-79e4-4fe0-ab44-eec2f41cae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from training_dataset_baseline.csv...\n",
      "\n",
      "Detected 193 feature columns.\n",
      "\n",
      "Class Distribution:\n",
      "label\n",
      "0    0.715196\n",
      "1    0.284804\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training size: 12409\n",
      "Test size: 5319\n",
      "\n",
      "Training Random Forest model...\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Wildlife       0.82      0.95      0.88      3804\n",
      "    Wildlife       0.80      0.46      0.58      1515\n",
      "\n",
      "    accuracy                           0.81      5319\n",
      "   macro avg       0.81      0.71      0.73      5319\n",
      "weighted avg       0.81      0.81      0.79      5319\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[3629  175]\n",
      " [ 821  694]]\n",
      "Confusion matrix plot saved to confusion_matrix.png\n",
      "\n",
      "Model saved to wildlife_model_rf.pkl\n",
      "Training pipeline complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "MODEL_OUTPUT_FILE = \"wildlife_model_rf.pkl\"\n",
    "PLOT_OUTPUT_FILE = \"confusion_matrix.png\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "    if not os.path.exists(DATASET_FILE):\n",
    "        print(f\"Error: Dataset file not found at {DATASET_FILE}\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv(DATASET_FILE)\n",
    "    df.fillna(0, inplace=True)  # Safety\n",
    "\n",
    "    # --- Identify label column ---\n",
    "    if \"label\" not in df.columns:\n",
    "        raise ValueError(\"ERROR: The dataset must include a 'label' column!\")\n",
    "\n",
    "    label_col = \"label\"\n",
    "    feature_cols = [col for col in df.columns if col not in [label_col, \"image_name\", \"cell_index\"]]\n",
    "\n",
    "    print(f\"\\nDetected {len(feature_cols)} feature columns.\")\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y = df[label_col]\n",
    "\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(y.value_counts(normalize=True))\n",
    "\n",
    "    # --- Train-test Split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining size: {len(X_train)}\")\n",
    "    print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "    # --- Model Training ---\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"No Wildlife\", \"Wildlife\"]))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "                yticklabels=[\"Act 0\", \"Act 1\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(PLOT_OUTPUT_FILE)\n",
    "    print(f\"Confusion matrix plot saved to {PLOT_OUTPUT_FILE}\")\n",
    "\n",
    "    # --- Save Model ---\n",
    "    with open(MODEL_OUTPUT_FILE, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"\\nModel saved to {MODEL_OUTPUT_FILE}\")\n",
    "    print(\"Training pipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "710b5088-d03b-4aef-a60a-04f61b4ad3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from training_dataset_baseline.csv...\n",
      "\n",
      "Detected 193 feature columns.\n",
      "\n",
      "Class Distribution:\n",
      "label\n",
      "0    0.715196\n",
      "1    0.284804\n",
      "Name: proportion, dtype: float64\n",
      "Training on 12409 samples, testing on 5319 samples.\n",
      "\n",
      "Scaling data for parametric/non-parametric models...\n",
      "Data scaling complete.\n",
      "Imbalance ratio (0s/1s): 2.51\n",
      "\n",
      "--- Starting Model Comparison ---\n",
      "\n",
      "Training LogisticRegression (Parametric)...\n",
      "\n",
      "Training MLPClassifier (Neural Net)...\n",
      "\n",
      "Training XGBoost (Tree-based)...\n",
      "\n",
      "Training Random Forest (Tree-based)...\n",
      "\n",
      "Training KNeighborsClassifier (Non-Parametric)...\n",
      "\n",
      "Training SVC (Parametric)...\n",
      "\n",
      "--- Final Model Comparison Results ---\n",
      "                                       Precision (for 1)  Recall (for 1)  \\\n",
      "Model                                                                      \n",
      "XGBoost (Tree-based)                            0.725050        0.724092   \n",
      "SVC (Parametric)                                0.646957        0.736634   \n",
      "MLPClassifier (Neural Net)                      0.730985        0.602640   \n",
      "LogisticRegression (Parametric)                 0.555778        0.733333   \n",
      "Random Forest (Tree-based)                      0.798619        0.458086   \n",
      "KNeighborsClassifier (Non-Parametric)           0.755981        0.312871   \n",
      "\n",
      "                                       F1-Score (for 1)  Train Time (sec)  \n",
      "Model                                                                      \n",
      "XGBoost (Tree-based)                           0.724571          1.787455  \n",
      "SVC (Parametric)                               0.688889         48.890001  \n",
      "MLPClassifier (Neural Net)                     0.660637          3.524493  \n",
      "LogisticRegression (Parametric)                0.632328          0.802845  \n",
      "Random Forest (Tree-based)                     0.582215          3.184368  \n",
      "KNeighborsClassifier (Non-Parametric)          0.442577          0.473922  \n",
      "\n",
      "Comparison report saved to model_comparison_report.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import All the Models ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "df = pd.read_csv(DATASET_FILE)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- Identify label column ---\n",
    "if \"label\" not in df.columns:\n",
    "    raise ValueError(\"ERROR: The dataset must include a 'label' column!\")\n",
    "\n",
    "label_col = \"label\"\n",
    "feature_cols = [col for col in df.columns if col not in ['image_name', 'cell_index', 'label']]\n",
    "\n",
    "print(f\"\\nDetected {len(feature_cols)} feature columns.\")\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[label_col]\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# --- Train-test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
    "\n",
    "# --- 3. Create Scaled Data (CRITICAL STEP) ---\n",
    "print(\"\\nScaling data for parametric/non-parametric models...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "# --- 4. Handle Class Imbalance (for XGBoost) ---\n",
    "count_0 = y_train.value_counts()[0]\n",
    "count_1 = y_train.value_counts()[1]\n",
    "scale_pos_weight = count_0 / count_1\n",
    "print(f\"Imbalance ratio (0s/1s): {scale_pos_weight:.2f}\")\n",
    "\n",
    "# --- 5. Define Models ---\n",
    "# Note: \"Linear Regression\" and \"SVR\" are for regression.\n",
    "# Their classification counterparts are \"LogisticRegression\" and \"SVC\".\n",
    "models = {\n",
    "    \"LogisticRegression (Parametric)\": LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    \n",
    "    \"MLPClassifier (Neural Net)\": MLPClassifier(hidden_layer_sizes=(100, 50), # 2 hidden layers\n",
    "                                                max_iter=300, \n",
    "                                                random_state=42, \n",
    "                                                early_stopping=True),\n",
    "                                                \n",
    "    \"XGBoost (Tree-based)\": XGBClassifier(scale_pos_weight=scale_pos_weight, \n",
    "                                          random_state=42, \n",
    "                                          n_jobs=-1,\n",
    "                                          eval_metric='logloss'),\n",
    "\n",
    "    \"Random Forest (Tree-based)\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "\n",
    "    # --- WARNING: SLOW MODELS ---\n",
    "    # The models below (KNN, SVC) are EXTREMELY slow on large,\n",
    "    # high-dimensional data. Run them at your own risk.\n",
    "    # I have commented them out so you don't accidentally run them.\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \"KNeighborsClassifier (Non-Parametric)\": KNeighborsClassifier(n_neighbors=7, n_jobs=-1),\n",
    "    \n",
    "    \"SVC (Parametric)\": SVC(class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "# Define which models need scaled data\n",
    "models_that_need_scaling = [\"LogisticRegression (Parametric)\", \n",
    "                            \"MLPClassifier (Neural Net)\", \n",
    "                            \"KNeighborsClassifier (Non-Parametric)\", \n",
    "                            \"SVC (Parametric)\"]\n",
    "\n",
    "# --- 6. Train and Evaluate ---\n",
    "results = []\n",
    "print(\"\\n--- Starting Model Comparison ---\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Choose the correct dataset (scaled or unscaled)\n",
    "    if model_name in models_that_need_scaling:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        # Tree models (XGBoost, RandomForest) use unscaled data\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Get the classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Extract scores for the \"Wildlife (1)\" class\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1_score = report['1']['f1-score']\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision (for 1)\": precision,\n",
    "        \"Recall (for 1)\": recall,\n",
    "        \"F1-Score (for 1)\": f1_score,\n",
    "        \"Train Time (sec)\": end_time - start_time\n",
    "    })\n",
    "\n",
    "# --- 7. Print Final Comparison Table ---\n",
    "print(\"\\n--- Final Model Comparison Results ---\")\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1-Score (for 1)\", ascending=False)\n",
    "results_df = results_df.set_index(\"Model\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to a file for your presentation\n",
    "results_df.to_csv(\"model_comparison_report.csv\")\n",
    "print(\"\\nComparison report saved to model_comparison_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6d44395-fcc0-4e5f-a2a6-1059f07f7d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from training_dataset_baseline.csv...\n",
      "Data loaded and cleaned (NaNs filled with 0).\n",
      "Index(['B_mean', 'G_mean', 'R_mean', 'B_std', 'G_std', 'R_std', 'Edge_density',\n",
      "       'LBP_bin_0', 'LBP_bin_1', 'LBP_bin_2',\n",
      "       ...\n",
      "       'hog_89', 'hog_90', 'hog_91', 'hog_92', 'hog_93', 'hog_94', 'hog_95',\n",
      "       'label', 'image_name', 'cell_index'],\n",
      "      dtype='object', length=196)\n",
      "Detected 193 feature columns.\n",
      "Splitting data by image...\n",
      "Training on 14144 samples, testing on 3584 samples.\n",
      "Training Stage 1 Model (XGBClassifier) on 193 features...\n",
      "Stage 1 training complete.\n",
      "Stage 1 model saved to stage1_model.pkl\n",
      "Best threshold: 0.39 F1: 0.692131398013751\n",
      "Saved Stage-1 feature matrices (with stage1_prob).\n",
      "\n",
      "--- Stage 1 Model Evaluation ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "No Wildlife (0)       0.92      0.74      0.82      2526\n",
      "   Wildlife (1)       0.58      0.86      0.69      1058\n",
      "\n",
      "       accuracy                           0.78      3584\n",
      "      macro avg       0.75      0.80      0.76      3584\n",
      "   weighted avg       0.82      0.78      0.78      3584\n",
      "\n",
      "\n",
      "Generating Stage 2 (context-aware) features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context features: 100%|███████████████████████████████████████████████████████████████| 221/221 [00:00<00:00, 274.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Stage 2 Model (RandomForest) on 9 features...\n",
      "Stage 2 training complete.\n",
      "Stage 2 model saved to stage2_model.pkl\n",
      "\n",
      "--- Stage 2 Model Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context features: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 366.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "No Wildlife (0)       0.87      0.86      0.86      2527\n",
      "   Wildlife (1)       0.67      0.68      0.68      1057\n",
      "\n",
      "       accuracy                           0.81      3584\n",
      "      macro avg       0.77      0.77      0.77      3584\n",
      "   weighted avg       0.81      0.81      0.81      3584\n",
      "\n",
      "train\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "No Wildlife (0)       1.00      1.00      1.00     10157\n",
      "   Wildlife (1)       1.00      1.00      1.00      3987\n",
      "\n",
      "       accuracy                           1.00     14144\n",
      "      macro avg       1.00      1.00      1.00     14144\n",
      "   weighted avg       1.00      1.00      1.00     14144\n",
      "\n",
      "\n",
      "All tasks complete. Both models and all plots/reports are generated.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = \"training_dataset_baseline.csv\"\n",
    "STAGE1_MODEL_FILE = \"stage1_model.pkl\" # Hand-crafted feature model\n",
    "STAGE2_MODEL_FILE = \"stage2_model.pkl\" # Context-aware model\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(f\"Loading dataset from {DATASET_FILE}...\")\n",
    "df = pd.read_csv(DATASET_FILE)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#df.dropna(inplace=True)\n",
    "# This ensures we keep all 64 cells for every image.\n",
    "df.fillna(0, inplace=True)\n",
    "print(\"Data loaded and cleaned (NaNs filled with 0).\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Get all unique feature names\n",
    "feature_names = [col for col in df.columns if col not in ['image_name', 'cell_index', 'label']]\n",
    "print(f\"Detected {len(feature_names)} feature columns.\")\n",
    "if 'Edge_density' not in feature_names:\n",
    "    print(\"FATAL ERROR: 'Edge_density' feature not found, which is needed for Stage 2.\")\n",
    "    # This is a safety check\n",
    "\n",
    "\"\"\"# --- Identify label column ---\n",
    "if \"label\" not in df.columns:\n",
    "    raise ValueError(\"ERROR: The dataset must include a 'label' column!\")\n",
    "\n",
    "label_col = \"label\"\n",
    "[col for col in df.columns if col not in ['image_name', 'cell_index', 'label']]\n",
    "\n",
    "print(f\"\\nDetected {len(feature_cols)} feature columns.\")\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[label_col]\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# --- Train-test Split ---\n",
    "X_train_stage1, X_test_stage1, y_train_stage1, y_test_stage1 = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\"\"\"\n",
    "\n",
    "print(\"Splitting data by image...\")\n",
    "image_names = df['image_name'].unique()\n",
    "train_names, test_names = train_test_split(image_names, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df['image_name'].isin(train_names)].reset_index(drop=True)\n",
    "test_df  = df[df['image_name'].isin(test_names)].reset_index(drop=True)\n",
    "\n",
    "# Prepare Stage 1 training and test sets\n",
    "X_train_stage1 = train_df[feature_names]\n",
    "y_train_stage1 = train_df['label']\n",
    "X_test_stage1 = test_df[feature_names]\n",
    "y_test_stage1 = test_df['label']\n",
    "print(f\"Training on {len(X_train_stage1)} samples, testing on {len(X_test_stage1)} samples.\")\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_stage1, y_train_stage1)\n",
    "\n",
    "# --- 3. Train Stage 1 Model (Hand-crafted Features) ---\n",
    "print(f\"Training Stage 1 Model (XGBClassifier) on {len(feature_names)} features...\")\n",
    "model_stage1 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "#model_stage1 = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, n_jobs=-1,eval_metric='logloss')\n",
    "model_stage1.fit(X_res, y_res)\n",
    "print(\"Stage 1 training complete.\")\n",
    "\n",
    "# Save Stage 1 model\n",
    "with open(STAGE1_MODEL_FILE, 'wb') as f:\n",
    "    pickle.dump(model_stage1, f)\n",
    "print(f\"Stage 1 model saved to {STAGE1_MODEL_FILE}\")\n",
    "\n",
    "train_df[\"stage1_prob\"] = model_stage1.predict_proba(train_df[feature_names])[:,1]\n",
    "test_df[\"stage1_prob\"]  = model_stage1.predict_proba(test_df[feature_names])[:,1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_t = 0.5\n",
    "\n",
    "for t in [i/100 for i in range(5, 50)]:\n",
    "    preds = (test_df[\"stage1_prob\"] > t).astype(int)\n",
    "    f = f1_score(y_test_stage1, preds)\n",
    "    if f > best_f1:\n",
    "        best_f1 = f\n",
    "        best_t = t\n",
    "\n",
    "print(\"Best threshold:\", best_t, \"F1:\", best_f1)\n",
    "\n",
    "# Save extracted features to CSV\n",
    "train_df.to_csv(\"stage1_train_features.csv\", index=False)\n",
    "test_df.to_csv(\"stage1_test_features.csv\", index=False)\n",
    "\n",
    "print(\"Saved Stage-1 feature matrices (with stage1_prob).\")\n",
    "# --- 4. Evaluate Stage 1 Model ---\n",
    "print(\"\\n--- Stage 1 Model Evaluation ---\")\n",
    "y_pred_stage1 = (model_stage1.predict_proba(X_test_stage1)[:,1] > best_t).astype(int)\n",
    "report_stage1 = classification_report(y_test_stage1, y_pred_stage1, target_names=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "print(report_stage1)\n",
    "with open(\"classification_report.txt\", \"w\") as f:\n",
    "    f.write(\"--- STAGE 1 (Hand-crafted Features Only) ---\\n\")\n",
    "    f.write(report_stage1)\n",
    "\n",
    "cm_stage1 = confusion_matrix(y_test_stage1, y_pred_stage1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_stage1, annot=True, fmt='d', cmap='Blues', xticklabels=['No Wildlife (0)', 'Wildlife (1)'], yticklabels=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "plt.title('Stage 1 (Hand-crafted) Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(\"confusion_matrix_stage1.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 5. Create Stage 2 Dataset (Context-Aware) ---\n",
    "print(\"\\nGenerating Stage 2 (context-aware) features...\")\n",
    "\n",
    "def create_stage2_features(image_df, stage1_model):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame for one or more images, gets Stage 1 probabilities,\n",
    "    and builds the 9-feature (self + 8 neighbors) dataset.\n",
    "    \"\"\"\n",
    "    # Get Stage 1 probabilities (for class 1)\n",
    "    stage1_probs = stage1_model.predict_proba(image_df[feature_names])[:, 1]\n",
    "    image_df['stage1_prob'] = stage1_probs\n",
    "    \n",
    "    stage2_features = []\n",
    "    stage2_labels = []\n",
    "    \n",
    "    for image_name in tqdm(image_df['image_name'].unique(), desc=\"Building context features\"):\n",
    "        img_cells = image_df[image_df['image_name'] == image_name].sort_values('cell_index')\n",
    "        \n",
    "        # Create 8x8 grid of probabilities\n",
    "        prob_grid = img_cells.set_index('cell_index')['stage1_prob'].reindex(range(64), fill_value=0.0).values.reshape(8, 8)\n",
    "        label_grid = img_cells.set_index('cell_index')['label'].reindex(range(64), fill_value=0.0).values.reshape(8, 8)\n",
    "        edge_grid = img_cells.set_index('cell_index')['Edge_density'].reindex(range(64), fill_value=0.0).values.reshape(8, 8)\n",
    "        \n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                # 10 features: 9 probabilities + 1 edge density\n",
    "                features = np.zeros(10)\n",
    "                features[0] = prob_grid[r, c] # Self prob\n",
    "                \n",
    "                # Get neighbor probabilities, using 0.0 for padding at edges\n",
    "                for i, (dr, dc) in enumerate([(-1, -1), (-1, 0), (-1, 1),\n",
    "                                               ( 0, -1),           ( 0, 1),\n",
    "                                               ( 1, -1), ( 1, 0), ( 1, 1)]):\n",
    "                    nr, nc = r + dr, c + dc\n",
    "                    if 0 <= nr < 8 and 0 <= nc < 8:\n",
    "                        features[i+1] = prob_grid[nr, nc]\n",
    "                # --- Add the cell's own Edge_density as the 10th feature ---\n",
    "                features[9] = edge_grid[r, c]\n",
    "                stage2_features.append(features)\n",
    "                stage2_labels.append(label_grid[r, c])\n",
    "                \n",
    "    return np.array(stage2_features), np.array(stage2_labels)\n",
    "\n",
    "X_train_stage2, y_train_stage2 = create_stage2_features(train_df, model_stage1)\n",
    "\n",
    "# --- 6. Train Stage 2 Model (Context-Aware) ---\n",
    "print(f\"\\nTraining Stage 2 Model (RandomForest) on 9 features...\")\n",
    "# Using a simple model here is best. RandomForest would also work.\n",
    "# model_stage2 = LogisticRegression(random_state=42, class_weight='balanced') # <-- OLD MODEL\n",
    "model_stage2 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced') # <-- NEW, MORE POWERFUL MODEL\n",
    "model_stage2.fit(X_train_stage2, y_train_stage2)\n",
    "print(\"Stage 2 training complete.\")\n",
    "\n",
    "# Save Stage 2 model\n",
    "with open(STAGE2_MODEL_FILE, 'wb') as f:\n",
    "    pickle.dump(model_stage2, f)\n",
    "print(f\"Stage 2 model saved to {STAGE2_MODEL_FILE}\")\n",
    "\n",
    "# --- 7. Evaluate Stage 2 Model ---\n",
    "print(\"\\n--- Stage 2 Model Evaluation ---\")\n",
    "# Create test features\n",
    "X_test_stage2, y_test_stage2 = create_stage2_features(test_df, model_stage1)\n",
    "y_pred_stage2 = model_stage2.predict(X_test_stage2)\n",
    "y_pred_stage2_train = model_stage2.predict(X_train_stage2)\n",
    "\n",
    "print(\"test\")\n",
    "report_stage2 = classification_report(y_test_stage2, y_pred_stage2, target_names=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "print(report_stage2)\n",
    "\n",
    "print(\"train\")\n",
    "report_stage2_train = classification_report(y_train_stage2, y_pred_stage2_train, target_names=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "print(report_stage2_train)\n",
    "\n",
    "with open(\"classification_report.txt\", \"a\") as f: # Append to the report file\n",
    "    f.write(\"\\n\\n--- STAGE 2 (Context-Aware) ---\\n\")\n",
    "    f.write(report_stage2)\n",
    "\n",
    "cm_stage2 = confusion_matrix(y_test_stage2, y_pred_stage2)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_stage2, annot=True, fmt='d', cmap='Blues', xticklabels=['No Wildlife (0)', 'Wildlife (1)'], yticklabels=['No Wildlife (0)', 'Wildlife (1)'])\n",
    "plt.title('Stage 2 (Context-Aware) Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(\"confusion_matrix_stage2.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll tasks complete. Both models and all plots/reports are generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69abe98-999f-462f-8b04-2c7516ffc2e7",
   "metadata": {},
   "source": [
    "EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1c726-64af-441b-b1a4-4d029900e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from stage1_model.pkl...\n",
      "Loading model from stage2_model.pkl...\n",
      "Output images will be saved to predicted_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   1%|▊                                                                         | 3/277 [00:07<12:03,  2.64s/it]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from skimage.feature import local_binary_pattern\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "STAGE1_MODEL_FILE = \"stage1_model.pkl\"         # Your saved .pkl model file\n",
    "STAGE2_MODEL_FILE = \"stage2_model.pkl\"         # Your saved .pkl model file\n",
    "INPUT_IMAGE_DIR = \"preprocessed_images\"     # Folder of new, unseen images\n",
    "OUTPUT_IMAGE_DIR = \"predicted_images\"     # Folder for highlighted images\n",
    "OUTPUT_CSV_FILE = \"predictions.csv\"       # The final CSV output\n",
    "\n",
    "# --- Grid & Preprocessing Constants ---\n",
    "TARGET_WIDTH = 800\n",
    "TARGET_HEIGHT = 600\n",
    "TARGET_ASPECT_RATIO = TARGET_WIDTH / TARGET_HEIGHT\n",
    "GRID_ROWS = 8\n",
    "GRID_COLS = 8\n",
    "CELL_HEIGHT = TARGET_HEIGHT // GRID_ROWS # 75\n",
    "CELL_WIDTH = TARGET_WIDTH // GRID_COLS   # 100\n",
    "\n",
    "\n",
    "# --- MAIN PREDICTION SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. Load Model ---\n",
    "    print(f\"Loading model from {STAGE1_MODEL_FILE}...\")\n",
    "    if not os.path.exists(STAGE1_MODEL_FILE):\n",
    "        print(f\"Error: Model file {STAGE1_MODEL_FILE} not found.\")\n",
    "        print(\"Please run model_trainer.py first.\")\n",
    "        exit()\n",
    "    print(f\"Loading model from {STAGE2_MODEL_FILE}...\")\n",
    "    if not os.path.exists(STAGE2_MODEL_FILE):\n",
    "        print(f\"Error: Model file {STAGE2_MODEL_FILE} not found.\")\n",
    "        print(\"Please run model_trainer.py first.\")\n",
    "        exit()\n",
    "    # Load Stage 1 & Stage 2 models\n",
    "    with open(STAGE1_MODEL_FILE,\"rb\") as f:\n",
    "        model_stage1 = pickle.load(f)\n",
    "    with open(STAGE2_MODEL_FILE,\"rb\") as f:\n",
    "        model_stage2 = pickle.load(f)\n",
    "\n",
    "\n",
    "    # --- 2. Setup Directories ---\n",
    "    if not os.path.exists(INPUT_IMAGE_DIR):\n",
    "        print(f\"Error: Input directory {INPUT_IMAGE_DIR} not found.\")\n",
    "        print(\"Please create it and add images you want to predict.\")\n",
    "        exit()\n",
    "        \n",
    "    os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "    print(f\"Output images will be saved to {OUTPUT_IMAGE_DIR}\")\n",
    "    \n",
    "    image_paths = glob.glob(os.path.join(INPUT_IMAGE_DIR, '*.jpg'))\n",
    "    image_paths += glob.glob(os.path.join(INPUT_IMAGE_DIR, '*.png'))\n",
    "    image_paths += glob.glob(os.path.join(INPUT_IMAGE_DIR, '*.jpeg'))\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"Error: No images found in {INPUT_IMAGE_DIR}.\")\n",
    "        exit()\n",
    "\n",
    "    prob_grid = np.zeros((GRID_ROWS, GRID_COLS))\n",
    "    # --- 3. Run Prediction Loop ---\n",
    "    all_csv_rows = []\n",
    "    \n",
    "    for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        image_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Load the image\n",
    "        processed_image = cv2.imread(image_path)\n",
    "        if processed_image is None:\n",
    "            print(f\"Warning: Skipping invalid image {image_name}\")\n",
    "            continue\n",
    "        \n",
    "        # This copy is for drawing our highlights on\n",
    "        output_image = processed_image.copy()\n",
    "        \n",
    "        # This list will hold 'c01', 'c02', ... predictions\n",
    "        csv_row = [image_name]\n",
    "        \n",
    "        # Iterate over the 8x8 grid\n",
    "        all_cells_features = []\n",
    "        for i in range(GRID_ROWS):\n",
    "            for j in range(GRID_COLS):\n",
    "                # Extract the 100x75 cell\n",
    "                y1, y2 = i * CELL_HEIGHT, (i + 1) * CELL_HEIGHT\n",
    "                x1, x2 = j * CELL_WIDTH, (j + 1) * CELL_WIDTH\n",
    "                cell = processed_image[y1:y2, x1:x2]\n",
    "                \n",
    "                # --- CORE ML PIPELINE ---\n",
    "                # 1. Extract features\n",
    "                all_cells_features.append(extract_features_for_cell(cell))\n",
    "\n",
    "        feature_names = [col for col in df.columns if col not in ['image_name', 'cell_index', 'label']]\n",
    "        features_df = pd.DataFrame(all_cells_features, columns=feature_names)\n",
    "        stage1_probs = model_stage1.predict_proba(features_df)[:, 1]\n",
    "        prob_grid = stage1_probs.reshape(GRID_ROWS, GRID_COLS)\n",
    "        \n",
    "        # Now run Stage-2 context model\n",
    "        csv_row = [image_name]\n",
    "        \n",
    "        # Create 8x8 grid of probabilities\n",
    "        pred_grid = np.zeros((8,8), dtype=int)\n",
    "\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                features2 = []\n",
    "                features2.append(prob_grid[r,c])   # center prob\n",
    "        \n",
    "                # neighbors\n",
    "                for dr,dc in neighbors:\n",
    "                    rr = r + dr\n",
    "                    cc = c + dc\n",
    "                    if 0 <= rr < 8 and 0 <= cc < 8:\n",
    "                        features2.append(prob_grid[rr,cc])\n",
    "                    else:\n",
    "                        features2.append(0)\n",
    "        \n",
    "                # add edge_density from features_df\n",
    "                cell_index = r*8 + c\n",
    "                features2.append(features_df.iloc[cell_index][\"Edge_density\"])\n",
    "                \n",
    "                pred = model_stage2.predict(np.array(features2).reshape(1,-1))[0]\n",
    "                pred_grid[r,c] = pred\n",
    "\n",
    "                csv_row.append(int(pred))\n",
    "                \n",
    "                # --- 4. Visualize Prediction ---\n",
    "                if pred == 1:\n",
    "                    x1, y1 = c * CELL_WIDTH, r * CELL_HEIGHT\n",
    "                    x2, y2 = (c + 1) * CELL_WIDTH, (r + 1) * CELL_HEIGHT\n",
    "                    overlay = output_image.copy()\n",
    "                    cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), -1)\n",
    "                    alpha = 0.4\n",
    "                    cv2.addWeighted(overlay, alpha, output_image, 1 - alpha, 0, output_image)\n",
    "                    \n",
    "        # Add this image's row of 64 predictions to our list\n",
    "        all_csv_rows.append(csv_row)\n",
    "        \n",
    "        # Save the highlighted image\n",
    "        output_path = os.path.join(OUTPUT_IMAGE_DIR, f\"pred_{image_name}\")\n",
    "        cv2.imwrite(output_path, output_image)\n",
    "\n",
    "    # --- 4. Save Final CSV Output ---\n",
    "    print(\"\\nSaving final predictions CSV...\")\n",
    "    \n",
    "    # Create column headers: 'image_name', 'c01', 'c02', ..., 'c64'\n",
    "    headers = ['image_name'] + [f'c{i:02d}' for i in range(1, (GRID_ROWS * GRID_COLS) + 1)]\n",
    "    \n",
    "    # Create and save the DataFrame\n",
    "    csv_df = pd.DataFrame(all_csv_rows, columns=headers)\n",
    "    csv_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "    \n",
    "    print(f\"Prediction complete. View highlighted images in '{OUTPUT_IMAGE_DIR}' and results in '{OUTPUT_CSV_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43f3e5-2c94-4ebb-942d-8faa6ebbcda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
